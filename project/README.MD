# 简历上所写项目潜在的知识点

### SIFT特征点的原理？

### SVM的原理和推导？

### Bag of Words图像分类的原理？

### Kmeans的原理？

### CNN的常用分类框架和特点？

### Faster RCNN的特点？

### 语义分割经典框架有哪些？特点是什么？

### 如何解决过拟合问题？
过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。具体表现就是最终模型在训练集上效果好；在测试集上效果差。模型泛化能力弱。

1. ReLu
2. dropout
3. 数据增广
4. BN（正则化）
5. 多模型融合
6 限制训练时间（early stopping）

### 自然场景文字定位用的是什么网络？
CTPN，EAST

### 切割算法怎么实现？有什么困难和优化方向？

### 仿射变换和透视变换的原理？

### 特征点匹配的具体实现？

### 自动驾驶系统的框架是如何的？线程同步和进程通信怎么做？

### 文字识别系统的框架是如何的？通信方式是什么？

### 霍夫变换的原理？

### OTSU的原理？

### 灰度直方图均衡化的原理？

### 膨胀与腐蚀的原理？

### 边缘检测的原理？

### 票据矫正的原理是？

### NMS原理？
非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。这里不讨论通用的NMS算法，而是用于在目标检测中用于提取分数最高的窗口的。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。
1. 将所有框的得分排序，选中最高分及其对应的框
2. 遍历其余的框，如果和当前最高分框的重叠面积(IOU)大于一定阈值，我们就将框删除。
3. 从未处理的框中继续选一个得分最高的，重复上述过程。

### 什么是梯度消失和梯度爆炸？怎么解决？
那么为什么会出现梯度消失的现象呢？因为通常神经网络所用的激活函数是sigmoid函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))f′(x)=f(x)(1−f(x))。因此两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。

那么什么是梯度爆炸呢？误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。 在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。 网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。 

解决：Relu,BN,ResNet

### L1和L2正则化的区别？
L1相比于L2，有所不同：
1. L1减少的是一个常量，L2减少的是权重的固定比例
2. 孰快孰慢取决于权重本身的大小，权重刚大时可能L2快，较小时L1快
3. L1使权重稀疏，L2使权重平滑，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0

实践中L2正则化通常优于L1正则化。

### Batch Normalization的步骤是？
![](./20180417154732.png)
